{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[Title] Good/Bad Prediction using MLP Classification with Tensorflow.Keras\n",
    "[Author] Yibeck Lee(yibec.Lee@gmail.com)\n",
    "[Contents]\n",
    " - Multi-Layer Perceptron Classification for Good/Bad Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    A simple neural network written in Keras (TensorFlow backend) to classify the IRIS data\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_iris() # load the iris dataset\n",
    "trainDf = pd.read_csv(\"../data_source/data01-train.csv\", header=None)\n",
    "#trainDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.721"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10000 - trainDf[1].sum())/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainDf.values\n",
    "\n",
    "trainFEATURES = train_data[:,4:]\n",
    "trainFEATURES[:5,]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(trainFEATURES)\n",
    "trainFEATURES= scaler.transform(trainFEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLABEL = train_data[:,1]\n",
    "trainLABEL = trainLABEL.reshape(-1,1)\n",
    "trainLABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "trainLABEL = encoder.fit_transform(trainLABEL)\n",
    "trainLABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(123, input_shape=(9,), activation='relu', name='fc1'))\n",
    "model.add(Dense(123, activation='relu', name='fc2'))\n",
    "model.add(Dense(123, activation='relu', name='fc3'))\n",
    "model.add(Dense(2, activation='softmax', name='output'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1a49c0cbdc3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Adam optimizer with learning rate of 0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adam optimizer with learning rate of 0.001\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Neural Network Model Summary: ')\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainFEATURES, trainLABEL, verbose=2, batch_size=100, epochs=100)\n",
    "\n",
    "# Test on unseen data\n",
    "\n",
    "results = model.evaluate(trainFEATURES, trainLABEL)\n",
    "\n",
    "print('Final test set loss: {:4f}'.format(results[0]))\n",
    "print('Final test set accuracy: {:4f}'.format(results[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_iris() # load the iris dataset\n",
    "testDf = pd.read_csv(\"../data_source/data01-test.csv\", header=None)\n",
    "#testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7183"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10000 - testDf[1].sum())/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = trainDf.values\n",
    "\n",
    "testFEATURES = test_data[:,4:]\n",
    "testFEATURES[:5,]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(testFEATURES)\n",
    "testFEATURES= scaler.transform(testFEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLABEL = test_data[:,1]\n",
    "testLABEL = testLABEL.reshape(-1,1)\n",
    "#testLABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "testLABEL = encoder.fit_transform(testLABEL)\n",
    "#testLABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "\n",
    "#model.add(Dense(256, input_shape=(9,), activation='relu', name='fc1'))\n",
    "#model.add(Dense(256, activation='relu', name='fc2'))\n",
    "#model.add(Dense(256, activation='relu', name='fc3'))\n",
    "#model.add(Dense(2, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model Summary: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc1 (Dense)                  (None, 123)               1230      \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 123)               15252     \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 123)               15252     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 248       \n",
      "=================================================================\n",
      "Total params: 31,982\n",
      "Trainable params: 31,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1495 - acc: 0.9399\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1409 - acc: 0.9423\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1359 - acc: 0.9447\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1329 - acc: 0.9462\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1324 - acc: 0.9467\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1292 - acc: 0.9467\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1262 - acc: 0.9482\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1235 - acc: 0.9494\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1226 - acc: 0.9493\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1192 - acc: 0.9508\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1154 - acc: 0.9529\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.1134 - acc: 0.9537\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.1144 - acc: 0.9521\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.1172 - acc: 0.9527\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.1084 - acc: 0.9559\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.1088 - acc: 0.9549\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.1025 - acc: 0.9588\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.1027 - acc: 0.9585\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1013 - acc: 0.9589\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0970 - acc: 0.9622\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0944 - acc: 0.9631\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0926 - acc: 0.9630\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0934 - acc: 0.9633\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0905 - acc: 0.9634\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0877 - acc: 0.9662\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0862 - acc: 0.9669\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0880 - acc: 0.9637\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0854 - acc: 0.9661\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0801 - acc: 0.9693\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0789 - acc: 0.9681\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0818 - acc: 0.9671\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0756 - acc: 0.9712\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0764 - acc: 0.9700\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0842 - acc: 0.9658\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0737 - acc: 0.9714\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0684 - acc: 0.9742\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0726 - acc: 0.9717\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0719 - acc: 0.9730\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0699 - acc: 0.9726\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0673 - acc: 0.9748\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0625 - acc: 0.9771\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0628 - acc: 0.9772\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0608 - acc: 0.9766\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0569 - acc: 0.9794\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0581 - acc: 0.9794\n"
     ]
    }
   ],
   "source": [
    "# Adam optimizer with learning rate of 0.001\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Neural Network Model Summary: ')\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainFEATURES, trainLABEL, verbose=2, batch_size=100, epochs=100)\n",
    "\n",
    "# Test on unseen data\n",
    "\n",
    "results = model.evaluate(testFEATURES, testLABEL)\n",
    "\n",
    "print('Final test set loss: {:4f}'.format(results[0]))\n",
    "print('Final test set accuracy: {:4f}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.75149214e-01, 2.48508211e-02],\n",
       "       [2.01376427e-09, 1.00000000e+00],\n",
       "       [9.85905111e-01, 1.40949357e-02],\n",
       "       ...,\n",
       "       [1.00000000e+00, 4.26575138e-12],\n",
       "       [9.99986649e-01, 1.33022013e-05],\n",
       "       [8.77361238e-01, 1.22638755e-01]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOD_BAD_PROB = model.predict(testFEATURES)\n",
    "GOOD_BAD_PROB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOD_BAD_PREDICTION = np.argmax(GOOD_BAD_PROB,axis=1)\n",
    "GOOD_BAD_PREDICTION[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.] 0 0\n",
      "[0. 1.] 1 1\n",
      "[1. 0.] 0 0\n",
      "[0. 1.] 1 1\n",
      "[1. 0.] 0 0\n",
      "[1. 0.] 0 0\n",
      "[0. 1.] 1 1\n",
      "[1. 0.] 0 0\n",
      "[0. 1.] 1 1\n",
      "[1. 0.] 0 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    print(testLABEL[i], np.argmax(testLABEL[i]), GOOD_BAD_PREDICTION[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    }
   ],
   "source": [
    "miss = np.argmax(testLABEL, axis=1) - GOOD_BAD_PREDICTION\n",
    "print(abs(miss).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
