{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[Title] MLP with CSV format inputs\n",
    "[Author] Yibeck Lee(Yibeck.Lee@gmail.com)\n",
    "[Program Code Name] e200-mlp-csv-mnist.py  \n",
    "[Description]\n",
    "  - Local System의 CSV format 데이터 이용\n",
    "  - Hadoop의 Flat 파일(csv, json, parquet 등)에 적용 가능\n",
    "  - Naming 표준화로 가독성 확보\n",
    "[History]\n",
    "  - 2019-05-01 : 최초 작성\n",
    "  - 2019-05-05 : Naming 표준화 개선\n",
    "[References]\n",
    "  - https://www.tensorflow.org\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[type(ndArrayTrainLabel)] <class 'numpy.ndarray'> [ndArrayTrainLabel.shape] (60000, 1)\n",
      "[numRowsTrain] 60000\n",
      "[numLabelClass] 10\n",
      "[numFeatures] 784\n",
      "[onehotTrainLabel[:1,]] [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "[ndArrayTrain.shape] (60000, 794)\n",
      "[onehotTestLabel[:1,]] [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "[ndArrayTest.shape] (10000, 794)\n",
      "[numBatchesPerEpoch] 600\n",
      "WARNING:tensorflow:From <ipython-input-2-b420f4b5e656>:117: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "[stepOfEpoch] 0 [lossTotalPerEpoch] 8921639.744500894 [lossMeanPerEpoch] 14869.399574168157\n",
      "[lossMeanTest] 6306.644531735885 [accuracyTest] 0.8698\n",
      "[stepOfEpoch] 1 [lossTotalPerEpoch] 3275311.0197737725 [lossMeanPerEpoch] 5458.851699622954\n",
      "[lossMeanTest] 4623.1599733396 [accuracyTest] 0.8831\n",
      "[stepOfEpoch] 2 [lossTotalPerEpoch] 2501115.171247042 [lossMeanPerEpoch] 4168.525285411736\n",
      "[lossMeanTest] 4374.539000131983 [accuracyTest] 0.8693\n",
      "[stepOfEpoch] 3 [lossTotalPerEpoch] 2088013.5390798452 [lossMeanPerEpoch] 3480.022565133075\n",
      "[lossMeanTest] 4301.2937922055535 [accuracyTest] 0.8604\n",
      "[stepOfEpoch] 4 [lossTotalPerEpoch] 1777196.7201410201 [lossMeanPerEpoch] 2961.994533568367\n",
      "[lossMeanTest] 3451.7967064938352 [accuracyTest] 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    onehotPredictedLabel = tf.nn.softmax(Hypothesis)\\n    predictedlabel = tf.argmax(onehotPredictedLabel, 1)\\n    for i in range(len(testLabel)):\\n      print(i\\n           ,  sess.run(tf.argmax(testLabel[i:i+1],1))\\n           ,  sess.run(tf.argmax(predictedLabel.eval(feed_dict={holderFeatures:testFeatures[i:i+1]}),1))\\n           )\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "#from numpy import genfromtxt\n",
    "\n",
    "# def convert(imgf, labelf, outf, n):\n",
    "#     f = open(imgf, \"rb\")\n",
    "#     o = open(outf, \"w\")\n",
    "#     l = open(labelf, \"rb\")\n",
    "#     f.read(16)\n",
    "#     l.read(8)\n",
    "#     images = []\n",
    "#     for i in range(n):\n",
    "#         image = [ord(l.read(1))]\n",
    "#         for j in range(28*28):\n",
    "#             image.append(ord(f.read(1)))\n",
    "#         images.append(image)\n",
    "#     for image in images:\n",
    "#         o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "#     f.close()\n",
    "#     o.close()\n",
    "#     l.close()\n",
    "# convert(\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\",\n",
    "#         \"mnist_train.csv\", 60000)\n",
    "# convert(\"t10k-images-idx3-ubyte\", \"t10k-labels-idx1-ubyte\",\n",
    "#         \"mnist_test.csv\", 10000)\n",
    "\n",
    "# X, y = loadlocal_mnist(\n",
    "#     images_path='train-images-idx3-ubyte'\n",
    "#   , labels_path='train-labels-idx1-ubyte') \n",
    "# np.savetxt(fname='mnist-train-features.csv'\n",
    "#   , X=X, delimiter=',', fmt='%d') \n",
    "# np.savetxt(fname='mnist-train-labels.csv', X=y, delimiter=',', fmt='%d')\n",
    "# X, y = loadlocal_mnist(\n",
    "#     images_path='t10k-images-idx3-ubyte'\n",
    "#   , labels_path='t10k-labels-idx1-ubyte') \n",
    "# np.savetxt(fname='mnist-test-features.csv', X=X, delimiter=',', fmt='%d') \n",
    "# np.savetxt(fname='mnist-test-labels.csv', X=y, delimiter=',', fmt='%d')\n",
    "\n",
    "# arrMnistTrainFeatures = genfromtxt('mnist-train-features.csv', delimiter=',')\n",
    "# arrMnistTrainLabel= genfromtxt('mnist-train-labels.csv', delimiter=',')\n",
    "# arrMnistTestFeatures= genfromtxt('t10k-train-features.csv', delimiter=',')\n",
    "# arrMnistTestLabel= genfromtxt('t10k-train-labels.csv', delimiter=',')\n",
    "\n",
    "dfTrainFeatures = pd.read_csv('mnist-train-features.csv',header=None)\n",
    "dfTrainLabel = pd.read_csv('mnist-train-labels.csv',header=None)\n",
    "dfTestFeatures = pd.read_csv('mnist-test-features.csv',header=None) \n",
    "dfTestLabel = pd.read_csv('mnist-test-labels.csv',header=None)\n",
    "\n",
    "ndArrayTrainFeatures = dfTrainFeatures.values\n",
    "ndArrayTrainLabel = dfTrainLabel.values\n",
    "print('[type(ndArrayTrainLabel)]', type(ndArrayTrainLabel), '[ndArrayTrainLabel.shape]',ndArrayTrainLabel.shape)\n",
    "numRowsTrain = ndArrayTrainLabel.shape[0]\n",
    "print('[numRowsTrain]', numRowsTrain)\n",
    "numLabelClass=len(np.unique(ndArrayTrainLabel))\n",
    "print('[numLabelClass]', numLabelClass)\n",
    "numFeatures=dfTrainFeatures.shape[1]\n",
    "print('[numFeatures]', ndArrayTrainFeatures.shape[1])\n",
    "\n",
    "featuresBeginPosition = numLabelClass\n",
    "\n",
    "onehotTrainLabel=np.eye(numLabelClass)[ndArrayTrainLabel].reshape(-1,numLabelClass)\n",
    "print('[onehotTrainLabel[:1,]]', onehotTrainLabel[:1,])\n",
    "ndArrayTrain=np.concatenate([onehotTrainLabel, ndArrayTrainFeatures], axis=1)\n",
    "print('[ndArrayTrain.shape]', ndArrayTrain.shape)\n",
    "\n",
    "\n",
    "ndArrayTestFeatures = dfTestFeatures.values\n",
    "ndArrayTestLabel = dfTestLabel.values\n",
    "onehotTestLabel=np.eye(numLabelClass)[ndArrayTestLabel].reshape(-1,numLabelClass)\n",
    "print('[onehotTestLabel[:1,]]', onehotTestLabel[:1,])\n",
    "ndArrayTest=np.concatenate([onehotTestLabel, ndArrayTestFeatures], axis=1)\n",
    "print('[ndArrayTest.shape]', ndArrayTest.shape)\n",
    "\n",
    "numEpoches = 20\n",
    "numRowsPerBatch = 100\n",
    "numBatchesPerEpoch = int(numRowsTrain / numRowsPerBatch)\n",
    "print('[numBatchesPerEpoch]', numBatchesPerEpoch)\n",
    "learningRate=0.001\n",
    "numNodesEachHiddenLayer={'numNodesH1':256, 'numNodesH2':256}\n",
    "\n",
    "holderFeatures = tf.placeholder(shape=[None, numFeatures], dtype=tf.float64, name='holderFeatures')\n",
    "holderLabel = tf.placeholder(shape=[None, numLabelClass], dtype=tf.float64, name='holderLabel')\n",
    "\n",
    "inputToHidden1Matrices = tf.nn.tanh(tf.random_normal([numFeatures, numNodesEachHiddenLayer['numNodesH1']], dtype=tf.float64))\n",
    "hidden1ToHidden2Matrices = tf.nn.tanh(tf.truncated_normal([numNodesEachHiddenLayer['numNodesH1'],numNodesEachHiddenLayer['numNodesH2']], dtype=tf.float64))\n",
    "hidden2ToOutputMatrices = tf.nn.tanh(tf.truncated_normal([numNodesEachHiddenLayer['numNodesH2'],numLabelClass], dtype=tf.float64))\n",
    "\n",
    "biasHidden1 = tf.zeros([numNodesEachHiddenLayer['numNodesH1']], dtype=tf.float64)\n",
    "biasHidden2 = tf.zeros([numNodesEachHiddenLayer['numNodesH2']], dtype=tf.float64)\n",
    "biasOutput = tf.zeros([numLabelClass], dtype=tf.float64)\n",
    "\n",
    "Weights={\n",
    "  'weightLayerInputToHidden1' : tf.Variable(inputToHidden1Matrices, dtype=tf.float64)\n",
    "  ,'hidden1ToHidden2Matrices': tf.Variable(hidden1ToHidden2Matrices, dtype=tf.float64)\n",
    "  ,'hidden2ToOutputMatrices': tf.Variable(hidden2ToOutputMatrices, dtype=tf.float64)\n",
    "}\n",
    "\n",
    "biases={\n",
    "  'biasHidden1' : tf.Variable(biasHidden1, dtype=tf.float64)\n",
    ", 'biasHidden2' : tf.Variable(biasHidden2, dtype=tf.float64)\n",
    ", 'biasOutput' : tf.Variable(biasOutput, dtype=tf.float64)\n",
    "}\n",
    "\n",
    "\n",
    "def mlpModel(featureInputs):\n",
    "  equationHidden1 = tf.add(tf.matmul(featureInputs, Weights['weightLayerInputToHidden1']), biases['biasHidden1'])\n",
    "  equationHidden2 = tf.add(tf.matmul(equationHidden1, Weights['hidden1ToHidden2Matrices']), biases['biasHidden2'])\n",
    "  equationOutput = tf.add(tf.matmul(equationHidden2, Weights['hidden2ToOutputMatrices']), biases['biasOutput'])\n",
    "  return equationOutput\n",
    "\n",
    "Hypothesis = mlpModel(featureInputs = holderFeatures)\n",
    "\n",
    "lossCrossEntropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Hypothesis, labels=holderLabel))\n",
    "optimizationGradientDescent = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(lossCrossEntropy)\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  for stepOfEpoch in range(5): #range(numBatchesPerEpoch):\n",
    "    lossTotalPerEpoch=0\n",
    "    for stepOfBatch in range(numBatchesPerEpoch):\n",
    "      batchRowsBegin = stepOfBatch * numRowsPerBatch \n",
    "      batchRowsEnd = min(stepOfBatch * numRowsPerBatch + numRowsPerBatch, numRowsTrain)\n",
    "      batchTrainFeatures=ndArrayTrain[batchRowsBegin:batchRowsEnd, featuresBeginPosition:]\n",
    "      batchTrainLabel =  ndArrayTrain[batchRowsBegin:batchRowsEnd, :featuresBeginPosition]\n",
    "      # print('[batchTrainLabel]\\n', batchTrainLabel)\n",
    "      _, lossPerBatch =sess.run(\n",
    "          [optimizationGradientDescent, lossCrossEntropy]\n",
    "        , feed_dict = {\n",
    "                holderFeatures : batchTrainFeatures\n",
    "            ,   holderLabel : batchTrainLabel\n",
    "          }\n",
    "      )\n",
    "\n",
    "      lossTotalPerEpoch += lossPerBatch \n",
    "      lossMeanPerEpoch = lossTotalPerEpoch / numBatchesPerEpoch\n",
    "    print('[stepOfEpoch]',stepOfEpoch, '[lossTotalPerEpoch]', lossTotalPerEpoch, '[lossMeanPerEpoch]', lossMeanPerEpoch)\n",
    "\n",
    "    testFeatures = ndArrayTest[:, featuresBeginPosition:]\n",
    "    testLabel = ndArrayTest[:, :featuresBeginPosition]\n",
    "    lossMeanTest = sess.run(lossCrossEntropy, feed_dict={holderFeatures : testFeatures, holderLabel : testLabel})\n",
    "\n",
    "    predictedLabel = tf.nn.softmax(Hypothesis)\n",
    "    correctPrediction = tf.equal(tf.argmax(predictedLabel, 1),tf.argmax(holderLabel, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPrediction, \"float\"))\n",
    "    accuracyTest = sess.run(accuracy, feed_dict = {holderFeatures:testFeatures, holderLabel:testLabel})\n",
    "    print('[lossMeanTest]',lossMeanTest,'[accuracyTest]',accuracyTest)\n",
    "\n",
    "\"\"\"\n",
    "    onehotPredictedLabel = tf.nn.softmax(Hypothesis)\n",
    "    predictedlabel = tf.argmax(onehotPredictedLabel, 1)\n",
    "    for i in range(len(testLabel)):\n",
    "      print(i\n",
    "           ,  sess.run(tf.argmax(testLabel[i:i+1],1))\n",
    "           ,  sess.run(tf.argmax(predictedLabel.eval(feed_dict={holderFeatures:testFeatures[i:i+1]}),1))\n",
    "           )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
